{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d07f05da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  # Imports Python's built-in os module.Provides functions to interact with the operating system \n",
    "from dotenv import load_dotenv  # Imports load_dotenv function from the dotenv module. This function loads environment variables from a .env file into the environment variables of the operating system.\n",
    "load_dotenv() # Loads environment variables from a .env file into the operating system's environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "867aa6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langsmith Tracking and tracing\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a80fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Qwen says:\n",
      " LangChain is a library of functions designed to be used in various programming languages. It provides built-in support for various operations, such as string manipulation, conditional statements, etc.\n",
      "\n",
      "With LangChain, developers can easily write reusable code that works across different programming languages. This not only improves productivity but also reduces the risk of introducing bugs into applications.\n",
      "\n",
      "In summary, LangChain is a library of functions designed to be used in various programming languages. It provides built-in support for various operations, such as string manipulation, conditional statements, etc.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama # Imports the Ollama class from the langchain.llms module. This class is used to interact with the Ollama LLMs (Large Language Models).\n",
    "\n",
    "# Load the Qwen 0.5B model\n",
    "llm = Ollama(model=\"qwen:0.5b\")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is LangChain and how is it used with LLMs?\"\n",
    "response = llm.invoke(question)\n",
    "\n",
    "print(\" Qwen says:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebec182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate # Imports the ChatPromptTemplate class from the langchain_core.prompts module. This class is used to create chat-based prompt templates for LLMs.\n",
    "# Create a chat prompt template\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "496d2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Qwen says:\n",
      " To become an AI engineer, you typically need a degree in computer science or related fields. This can include degrees from top universities such as MIT, Berkeley, or Harvard.\n",
      "\n",
      "Once you have completed your education, you will likely attend one of the many graduate programs available in computer science. These graduate programs are designed to train students for careers in the field of AI engineering.\n",
      "\n",
      "After completing a graduate program, you may be eligible for a bachelor's degree in computer science. This degree will provide you with a solid foundation in computer science and will be useful for your career as an AI engineer.\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm # Creates a chain by combining the prompt and the LLM. This chain will use the prompt to format the input before passing it to the LLM.\n",
    "# Ask a question using the chain\n",
    "response=chain.invoke({\"input\": \"tell me about how to become an AI engineer\"})\n",
    "print(\" Qwen says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b778e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To become an AI engineer, you will need a strong foundation in computer science and data analysis.\n",
      "\n",
      "One important aspect of becoming an AI engineer is the ability to write high-quality code that can be used by other developers.\n",
      "\n",
      "In addition to technical skills, AI engineers must also have a good understanding of machine learning principles and techniques. They should also be able to apply these principles and techniques to solve real-world problems.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser # Imports the StrOutputParser class from the langchain_core.output_parsers module. This class is used to parse the output of LLMs into a string format.\n",
    "# Create an output parser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"tell me about how to become an AI engineer.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30f8f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate   # Imports the PromptTemplate class from the langchain_core.prompts module. This class is used to create text-based prompt templates for LLMs.\n",
    "# Create a prompt template\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query in JSON format.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25da289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'langchain', 'description': 'A language translation system that can be integrated with various programming languages.', 'components': [{'name': 'translator'}, {'name': 'parser'}], 'dependencies': [{'name': 'translator'}, {'name': 'parser'}]}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"query\":\"tell me about langchain.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfef08",
   "metadata": {},
   "source": [
    "PromptTemplate\n",
    "# Purpose: Used for creating text-based prompts for LLMs.\n",
    "# Input: A single string template with variables.\n",
    "# Use case: For models that expect a single block of text as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e808a",
   "metadata": {},
   "source": [
    "ChatPromptTemplate\n",
    "# Purpose: Used for creating chat-based prompts (multi-turn conversations).\n",
    "# Input: A list of messages (roles like system, user, assistant) with variables.\n",
    "# Use case: For chat models (like OpenAI ChatGPT, Ollama chat models) that expect a sequence of messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
